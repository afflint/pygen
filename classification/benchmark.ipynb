{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import codecs\n",
    "import os\n",
    "import pymongo\n",
    "import numpy as np\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wikisearch(object):\n",
    "    \n",
    "    def __init__(self, out_folder, results=20):\n",
    "        self.folder = out_folder\n",
    "        self.results = results\n",
    "        self.queries = []\n",
    "        self.mapping = defaultdict(lambda: set())\n",
    "    \n",
    "    def search(self, query):\n",
    "        qi = len(self.queries)\n",
    "        self.queries.append(query)\n",
    "        results = wikipedia.search(query, results=self.results)\n",
    "        for result in results:\n",
    "            try:\n",
    "                page = wikipedia.page(result)\n",
    "                self.mapping[qi].add((page.pageid, page.url))\n",
    "            except wikipedia.exceptions.DisambiguationError as e:\n",
    "                for option in e.options:\n",
    "                    page = wikipedia.page(option)\n",
    "                    self.mapping[qi].add((page.pageid, page.url))\n",
    "    \n",
    "    def save(self, content=True):\n",
    "        \"\"\"\n",
    "        If content is set to False, gets page summary\n",
    "        \"\"\"\n",
    "        outdict = {}\n",
    "        for qi, qt in enumerate(self.queries):\n",
    "            pages = list(self.mapping[qi])\n",
    "            outdict[qi] = {'query': qt, 'page_ids': [x[0] for x in pages], \n",
    "                           'page_urls': [x[1] for x in pages]}\n",
    "            for page_id, page_url in pages:\n",
    "                page = wikipedia.page(pageid=page_id)\n",
    "                if content:\n",
    "                    text = page.content\n",
    "                else:\n",
    "                    text = page.summary\n",
    "                with codecs.open(os.sep.join([self.folder, page_id + '.txt']), 'wb', encoding='utf-8') as tout:\n",
    "                    tout.write(text)\n",
    "        with codecs.open(os.sep.join([self.folder, 'queries.json']), 'wb', encoding='utf-8') as jout:\n",
    "            json.dump(outdict, jout)           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image tagging\n",
    "Based on several collections of images harvested from google images and tagged by clarifi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTags(object):\n",
    "    \n",
    "    def __init__(self, db_name, collection, selection=None,\n",
    "                category='category', query='query',\n",
    "                tag='name', weight='value', url='path', doc_id='doc_id',\n",
    "                host='localhost'):\n",
    "        self.category, self.query, self.tag = category, query, tag\n",
    "        self.weight, self.url, self.doc_id = weight, url, doc_id\n",
    "        self.db = pymongo.MongoClient(host=host)[db_name]\n",
    "        self.images = self.db[collection]\n",
    "        if selection is None:\n",
    "            self.selection = {}\n",
    "        else:\n",
    "            self.selection = selection\n",
    "        group = {'$group': {'_id': None, \n",
    "                            'docs': {'$addToSet': '$' + self.url},\n",
    "                            'tags': {'$addToSet': '$' + self.tag}\n",
    "                           }}\n",
    "        match = {'$match': self.selection}\n",
    "        pipeline = [match, group]\n",
    "        for record in self.images.aggregate(pipeline):\n",
    "            self.docs = record['docs']\n",
    "            self.tags = record['tags']\n",
    "        self.M = np.zeros((len(self.docs), len(self.tags)))\n",
    "        for record in self.images.find(self.selection):\n",
    "            self.M[self.docs.index(record[self.url])][self.tags.index(record[self.tag])] = record[self.weight]\n",
    "    \n",
    "    @property\n",
    "    def idf(self):\n",
    "        i = np.array([np.log(float(self.M.shape[1]) / (np.count_nonzero(x) + 1)) for x in self.M.T])\n",
    "        return i\n",
    "    \n",
    "    @property\n",
    "    def tfidf(self):\n",
    "        return self.M * self.idf\n",
    "    \n",
    "    @property\n",
    "    def probs(self):\n",
    "        s = np.sum(self.M, axis=1)\n",
    "        return (self.M.T / s).T\n",
    "    \n",
    "    def get_url(self, doc_pos):\n",
    "        return self.docs[doc_pos]\n",
    "    \n",
    "    def tag_stream(self):\n",
    "        for i, row in enumerate(self.M):\n",
    "            tags = [self.tags[y] for y in np.where(row > 0)[0]]\n",
    "            yield tags\n",
    "        \n",
    "    def html(self, doc_pos, width=200, local=False):\n",
    "        url = self.get_url(doc_pos)\n",
    "        if local:\n",
    "            prefix = 'file://'\n",
    "        else:\n",
    "            prefix = ''\n",
    "        if url is None:\n",
    "            return \"\"\n",
    "        else:\n",
    "            return '<img src=\"{}{}\" style=\"width: {}px;\">'.format(prefix, url, width)\n",
    "        \n",
    "    def query_vector(self, query, as_probs=False, use_tfidf=False):\n",
    "        q = np.zeros(self.M.shape[1])\n",
    "        for t in query:\n",
    "            try:\n",
    "                q[self.tags.index(t)] += 1\n",
    "            except ValueError:\n",
    "                pass\n",
    "        if as_probs:\n",
    "            q /= q.sum()\n",
    "        elif use_tfidf:\n",
    "            q *= self.idf\n",
    "        return q\n",
    "    \n",
    "    def vector_search(self, query, distance, use_tfidf=False):\n",
    "        if use_tfidf:\n",
    "            candidates = self.tfidf\n",
    "        else:\n",
    "            candidates = self.M\n",
    "        results = []\n",
    "        for i, v in enumerate(candidates):\n",
    "            results.append((i, distance(query, v)))\n",
    "        return sorted(results, key=lambda x: x[1])\n",
    "    \n",
    "    def multinomial_search(self, query):\n",
    "        qw = np.where(query > 0)[0]\n",
    "        results = []\n",
    "        candidates = self.probs\n",
    "        for i, v in enumerate(candidates):\n",
    "            doc_p = []\n",
    "            for token in qw:\n",
    "                doc_p.append(np.log(1 + np.power(v[token], query[token])))\n",
    "            results.append((i, sum(doc_p)))\n",
    "        return sorted(results, key=lambda x: -x[1])\n",
    "    \n",
    "    def bm25(self, query, k=0.5, b=0.5):\n",
    "        qw = np.where(query > 0)[0]\n",
    "        results = []\n",
    "        candidates = self.M\n",
    "        lavg = np.array([np.count_nonzero(x) for x in self.M]).mean()\n",
    "        df = np.array([float(self.M.shape[1]) / (np.count_nonzero(x) + 1) for x in self.M.T])\n",
    "        for i, v in enumerate(candidates):\n",
    "            rsv = 0\n",
    "            for t in qw:\n",
    "                w = np.log((1+(df[t]*(k+1)*v[t])) / (((k*((1-b) + b*np.count_nonzero(v)/lavg)) + v[t])+1))\n",
    "                rsv += w\n",
    "            results.append((i,rsv))\n",
    "        return sorted(results, key=lambda x: -x[1])\n",
    "    \n",
    "    def generate_queries(self, n_queries, threshold=None):\n",
    "        \"\"\"\n",
    "        Exploits LDA to find topics that will be used as queries as follows\n",
    "        Each topic defines a query\n",
    "        Topic tags is the query (with their probabilities as relevance)\n",
    "        Topic docs is the ground truth (with doc probabilities as relevance)\n",
    "        \"\"\"\n",
    "        data = self.tfidf\n",
    "        lda = LDA(n_components=n_queries, learning_method='batch')\n",
    "        docs = lda.fit_transform(data)\n",
    "        queries = []\n",
    "        tags = lda.components_ / lda.components_.sum(axis=1)[:, np.newaxis]\n",
    "        for topic in range(0, n_queries):\n",
    "            tq = sorted([(self.tags[i], p) for i, p in enumerate(tags[topic])], key=lambda x: -x[1])\n",
    "            dq = sorted([(self.docs[i], p) for i, p in enumerate(docs.T[topic])], key=lambda x: -x[1])\n",
    "            if threshold is None:\n",
    "                ts = np.argmax(np.diff([x[1] for x in tq], n=2))\n",
    "                ds = np.argmax(np.diff([x[1] for x in dq], n=2))\n",
    "                queries.append((tq[:ts], dq[:ds])) \n",
    "            else:\n",
    "                queries.append((tq[:threshold[0]], \n",
    "                              [y for y in dq if y[1] >= threshold[1]]))\n",
    "        return queries, docs, tags\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
