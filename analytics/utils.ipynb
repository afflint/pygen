{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility classes for data management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analytics pipeline\n",
    "\n",
    "1. Data acquisition\n",
    "2. Data exploration\n",
    "3. Data Manipulation\n",
    "    - Enrich/transform variables data\n",
    "    - Split in training and test sets\n",
    "    - Separate features variables from target variables\n",
    "    - Data cleaning\n",
    "    - Handle categorical variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Download(object):\n",
    "    \n",
    "    @staticmethod\n",
    "    def fetch_tgz(base_url, tgz, local):\n",
    "        \"\"\"\n",
    "        Fetch tgz data from http\n",
    "        \n",
    "        :param str base_url: file url except the filename\n",
    "        :param str tgz: name of tgz file\n",
    "        :param str local: path of local directory where to save data\n",
    "        \"\"\"\n",
    "        if not os.path.isdir(local):\n",
    "            os.makedirs(local)\n",
    "        tgz_file = os.path.join(local, tgz)\n",
    "        full_url = \"/\".join([base_url, tgz])\n",
    "        urllib.request.urlretrieve(full_url, tgz_file)\n",
    "        tar = tarfile.open(tgz_file)\n",
    "        tar.extractall(path=local)\n",
    "        tar.close()\n",
    "        os.remove(tgz_file)\n",
    "    \n",
    "    @staticmethod\n",
    "    def fetch_json(base_url, json_file, local):\n",
    "        \"\"\"\n",
    "        Fetch json data from http\n",
    "        \n",
    "        :param str base_url: file url\n",
    "        :param str json_file: name of destination file\n",
    "        :param str local: path of local directory where to save data\n",
    "        \"\"\"\n",
    "        if not os.path.isdir(local):\n",
    "            os.makedirs(local)\n",
    "        urllib.request.urlretrieve(base_url, tgz_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategies to split the dataset in train and test sets\n",
    "Problem: split a dataset such that we have a train set and a dataset.\n",
    "\n",
    "### Strategy 1: <code>DataManager.random_test()</code>\n",
    "Generate a random index from a seed and use it to split the data. This is not consistent in case of update. \n",
    "\n",
    "### Strategy 2: <code>DataManager.hash_test()</code>\n",
    "Take the hash of one of the unique identifiers provided in data. Then select instances to be part of the test set according to the last byte of hash. Consistent with respect to the identifier.\n",
    "\n",
    "### Strategy 3: <code>DataManager.stratified_test()</code>\n",
    "Split the dataset in groups according to the distribution of value in one or more attributes. Then, get a sample randomly from each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingSet(object):\n",
    "    \n",
    "    def __init__(self, dataframe, test_size=0.2):\n",
    "        \"\"\"\n",
    "        Split dataframe in train and test set\n",
    "        \n",
    "        :param pandas dataframe: data\n",
    "        :param float test_size: fraction of the dataset to be provided as test\n",
    "        \"\"\"\n",
    "        self.data = dataframe\n",
    "        self.test = test_size\n",
    "        \n",
    "    def random_test(self):\n",
    "        \"\"\"\n",
    "        :return train_set, test_set\n",
    "        \"\"\"\n",
    "        np.random.seed(42)\n",
    "        indexes = np.random.permutation(self.data.shape[0])\n",
    "        test_size = int(self.data.shape[0] * self.test)\n",
    "        train_indexes, test_indexes = indexes[test_size:], indexes[:test_size]\n",
    "        return self.data.iloc[train_indexes], self.data.iloc[test_indexes]\n",
    "    \n",
    "    def hash_test(self, column=None, hashf=hashlib.md5):\n",
    "        \"\"\"\n",
    "        :param str column: col to use as unique id. If None an ID column is added.\n",
    "        :param function hashf: hash function to use\n",
    "        :return train_set, test_set\n",
    "        \"\"\"\n",
    "        if column is not None:\n",
    "            h = self.data\n",
    "        else:\n",
    "            h, column = self.data.reset_index(), 'index'\n",
    "        test_data = h[column].apply(\n",
    "            lambda id_: hashf(np.int64(id_)).digest()[-1] < \n",
    "            256 * self.test\n",
    "        )\n",
    "        return self.data.loc[~test_data], self.data.loc[test_data]\n",
    "    \n",
    "    def stratified_test(self, column, strata=10):\n",
    "        \"\"\"\n",
    "        :param str column: the col to use for strata\n",
    "        :param int strata: number of classes (kmeans is used to create classes)\n",
    "        :return train_set, test_set\n",
    "        \"\"\"\n",
    "        kmeans = KMeans(n_clusters=strata).fit(self.data[column].values.reshape(-1,1))\n",
    "        h = self.data.copy()\n",
    "        h['klasses'] = kmeans.labels_\n",
    "        split = StratifiedShuffleSplit(n_splits=1, test_size=self.test, random_state=42)\n",
    "        for train_i, test_i in split.split(h, h['klasses']):\n",
    "            train_set, test_set = self.data.loc[train_i], self.data.loc[test_i]\n",
    "        return train_set, test_set\n",
    "    \n",
    "    @staticmethod\n",
    "    def training_labels(training_df, labels):\n",
    "        \"\"\"\n",
    "        Separate features from target labels\n",
    "        \n",
    "        :param pandas dataframe training_df: the training dataframe\n",
    "        :param list labels: labels to be used as target\n",
    "        \"\"\"\n",
    "        return training_df.drop(labels, axis=1), training_df[labels].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning\n",
    "Data cleaning has the main goal of get rid of NULL values in data. Strategies:\n",
    "- Get rid of the row(s) $\\rightarrow$ <code>df.dropna(subset=[columns])</code>\n",
    "- Get rid of the column(s) $\\rightarrow$ <code>df.drop([columns], axis=1)</code>\n",
    "- Set NULL values to some value (e.g., mean, median) $\\rightarrow$ <code>df[columns].fillna(median, inplace=True)</code>\n",
    "\n",
    "Another option is to use <code>sklearn.preprocessing.Imputer</code> as follows.\n",
    "\n",
    "# Categorical data\n",
    "For categorical data we implment the 'one-hot encoding', which saves a binary vector for each possible value of categorical fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer, LabelBinarizer, StandardScaler\n",
    "\n",
    "\n",
    "class DataCleaner(object):\n",
    "    \n",
    "    def __init__(self, strategy='median', text_attributes=None):\n",
    "        \"\"\"\n",
    "        :param text_attributes: columns not containing numerical values\n",
    "        \"\"\"\n",
    "        self.imputer = Imputer(strategy=strategy)\n",
    "        self.text_attributes = text_attributes\n",
    "        \n",
    "    def fit(self, df):\n",
    "        if self.text_attributes is not None:\n",
    "            dfc = df.drop(self.text_attributes, axis=1)\n",
    "        else:\n",
    "            dfc = df.copy()\n",
    "        self.imputer.fit(dfc)\n",
    "        \n",
    "    def transform(self, df, y=None):\n",
    "        if self.text_attributes is not None:\n",
    "            text = df[self.text_attributes].copy()\n",
    "            dfc = df.drop(self.text_attributes, axis=1)\n",
    "        else:\n",
    "            text = None\n",
    "            dfc = df.copy()\n",
    "        dfc = pd.DataFrame(self.imputer.transform(dfc), columns=dfc.columns, index=dfc.index)\n",
    "        if text is not None:\n",
    "            dfc = dfc.join(text)\n",
    "        return dfc\n",
    "        \n",
    "    def fit_transform(self, df, y=None):\n",
    "        \"\"\"\n",
    "        :param pandas dataframe df: the dataframe\n",
    "        \"\"\"\n",
    "        self.fit(df)\n",
    "        return self.transform(df)\n",
    "    \n",
    "    \n",
    "class CategoricalData(object):\n",
    "    \n",
    "    def __init__(self, text_attribute):\n",
    "        \"\"\"\n",
    "        :param text_attributes: columns not containing numerical values\n",
    "        \"\"\"\n",
    "        self.encoder = LabelBinarizer()\n",
    "        self.text_attribute = text_attribute\n",
    "    \n",
    "    def fit(self, df):\n",
    "        self.encoder.fit(df[self.text_attribute].copy())\n",
    "        \n",
    "    def transform(self, df, y=None):\n",
    "        \"\"\"\n",
    "        :param pandas dataframe df: the dataframe\n",
    "        :param y: see https://goo.gl/PeoVZ1\n",
    "        \"\"\"\n",
    "        data = df[self.text_attribute].copy()\n",
    "        dfc = df.drop(self.text_attribute, axis=1)\n",
    "        hc = self.encoder.fit_transform(data)\n",
    "        for i, col in enumerate(self.encoder.classes_):\n",
    "            dfc[col] = hc[:,i]\n",
    "        return dfc\n",
    "    \n",
    "    def fit_transform(self, df, y=None):\n",
    "        \"\"\"\n",
    "        :param y: see https://goo.gl/PeoVZ1\n",
    "        \"\"\"\n",
    "        return self.transform(df)\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
