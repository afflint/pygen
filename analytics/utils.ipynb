{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility classes for data management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import hashlib\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Download(object):\n",
    "    \n",
    "    @staticmethod\n",
    "    def fetch_tgz(base_url, tgz, local):\n",
    "        \"\"\"\n",
    "        Fetch tgz data from http\n",
    "        \n",
    "        :param str base_url: file url except the filename\n",
    "        :param str tgz: name of tgz file\n",
    "        :param str local: path of local directory where to save data\n",
    "        \"\"\"\n",
    "        if not os.path.isdir(local):\n",
    "            os.makedirs(local)\n",
    "        tgz_file = os.path.join(local, tgz)\n",
    "        full_url = \"/\".join([base_url, tgz])\n",
    "        urllib.request.urlretrieve(full_url, tgz_file)\n",
    "        tar = tarfile.open(tgz_file)\n",
    "        tar.extractall(path=local)\n",
    "        tar.close()\n",
    "        os.remove(tgz_file)\n",
    "        \n",
    "    def fetch_json(base_url, json_file, local):\n",
    "        \"\"\"\n",
    "        Fetch json data from http\n",
    "        \n",
    "        :param str base_url: file url\n",
    "        :param str json_file: name of destination file\n",
    "        :param str local: path of local directory where to save data\n",
    "        \"\"\"\n",
    "        if not os.path.isdir(local):\n",
    "            os.makedirs(local)\n",
    "        urllib.request.urlretrieve(base_url, tgz_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategies to split the dataset in train and test sets\n",
    "Problem: split a dataset such that we have a train set and a dataset.\n",
    "\n",
    "### Strategy 1: <code>DataManager.random_test()</code>\n",
    "Generate a random index from a seed and use it to split the data. This is not consistent in case of update. \n",
    "\n",
    "### Strategy 2: <code>DataManager.hash_test()</code>\n",
    "Take the hash of one of the unique identifiers provided in data. Then select instances to be part of the test set according to the last byte of hash. Consistent with respect to the identifier.\n",
    "\n",
    "### Strategy 3: <code>DataManager.stratified_test()</code>\n",
    "Split the dataset in groups according to the distribution of value in one or more attributes. Then, get a sample randomly from each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation(object):\n",
    "    \n",
    "    def __init__(self, dataframe, test_size=0.2):\n",
    "        \"\"\"\n",
    "        Split dataframe in train and test set\n",
    "        \n",
    "        :param pandas dataframe: data\n",
    "        :param float test_size: fraction of the dataset to be provided as test\n",
    "        \"\"\"\n",
    "        self.data = dataframe\n",
    "        self.test = test_size\n",
    "        \n",
    "    def random_test(self):\n",
    "        \"\"\"\n",
    "        :return train_set, test_set\n",
    "        \"\"\"\n",
    "        np.random.seed(42)\n",
    "        indexes = np.random.permutation(self.data.shape[0])\n",
    "        test_size = int(self.data.shape[0] * self.test)\n",
    "        train_indexes, test_indexes = indexes[test_size:], indexes[:test_size]\n",
    "        return self.data.iloc[train_indexes], self.data.iloc[test_indexes]\n",
    "    \n",
    "    def hash_test(self, column=None, hashf=hashlib.md5):\n",
    "        \"\"\"\n",
    "        :param str column: col to use as unique id. If None an ID column is added.\n",
    "        :param function hashf: hash function to use\n",
    "        :return train_set, test_set\n",
    "        \"\"\"\n",
    "        if column is not None:\n",
    "            h = self.data\n",
    "        else:\n",
    "            h, column = self.data.reset_index(), 'index'\n",
    "        test_data = h[column].apply(\n",
    "            lambda id_: hashf(np.int64(id_)).digest()[-1] < 256 * self.test\n",
    "        )\n",
    "        return self.data.loc[~test_data], self.data.loc[test_data]\n",
    "    \n",
    "    def stratified_test(self, column, strata=10):\n",
    "        \"\"\"\n",
    "        :param str column: the col to use for strata\n",
    "        :param int strata: number of classes (kmeans is used to create classes)\n",
    "        :return train_set, test_set\n",
    "        \"\"\"\n",
    "        kmeans = KMeans(n_clusters=strata).fit(self.data[column].values.reshape(-1,1))\n",
    "        h = self.data.copy()\n",
    "        h['klasses'] = kmeans.labels_\n",
    "        split = StratifiedShuffleSplit(n_splits=1, test_size=self.test, random_state=42)\n",
    "        for train_i, test_i in split.split(h, h['klasses']):\n",
    "            train_set, test_set = self.data.loc[train_i], self.data.loc[test_i]\n",
    "        return train_set, test_set\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
